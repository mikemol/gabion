name: ci

on:
  push:
    branches:
      - stage
  workflow_dispatch:

permissions:
  contents: read

jobs:
  dataflow-audit:
    if: github.event_name != 'workflow_dispatch' || (github.ref == 'refs/heads/stage' && github.actor == github.repository_owner)
    runs-on: ubuntu-latest
    timeout-minutes: 70
    env:
      MISE_TRUSTED_CONFIG_PATHS: ${{ github.workspace }}
      # ~= 1 hour at current CI throughput (~18k ticks/ms in timeout profiles).
      GABION_LSP_TIMEOUT_TICKS: "65000000"
      GABION_LSP_TIMEOUT_TICK_NS: "1000000"
    steps:
      # Allow-listed actions: actions/checkout, jdx/mise-action
      - name: Checkout
        uses: actions/checkout@de0fac2e4500dabe0009e67214ff5f5447ce83dd
        with:
          fetch-depth: 0
      - name: Install mise
        uses: jdx/mise-action@6d1e696aa24c1aa1bcc1adea0212707c71ab78a8
      - name: Install toolchain
        run: mise install
      - name: Create venv
        run: |
          mise exec -- python -m venv .venv
          echo "VIRTUAL_ENV=$PWD/.venv" >> $GITHUB_ENV
          echo "$PWD/.venv/bin" >> $GITHUB_PATH
      - name: Install dependencies (locked)
        run: |
          .venv/bin/python -m pip install --upgrade pip uv
          .venv/bin/uv pip sync requirements.lock
          .venv/bin/uv pip install -e .
      - name: Restore previous dataflow resume checkpoint (best effort)
        env:
          GH_TOKEN: ${{ github.token }}
          GH_REPO: ${{ github.repository }}
          GH_REF_NAME: ${{ github.ref_name }}
          GH_RUN_ID: ${{ github.run_id }}
        run: |
          .venv/bin/python - <<'PYTHON'
          import io
          import json
          import os
          import urllib.request
          import zipfile
          from pathlib import Path

          token = os.environ.get("GH_TOKEN", "").strip()
          repo = os.environ.get("GH_REPO", "").strip()
          ref_name = os.environ.get("GH_REF_NAME", "").strip()
          current_run_id = os.environ.get("GH_RUN_ID", "").strip()
          if not token or not repo:
              print("GitHub token/repository unavailable; skipping checkpoint restore.")
              raise SystemExit(0)

          api_url = f"https://api.github.com/repos/{repo}/actions/artifacts?name=dataflow-report&per_page=20"
          req = urllib.request.Request(
              api_url,
              headers={
                  "Accept": "application/vnd.github+json",
                  "Authorization": f"Bearer {token}",
                  "X-GitHub-Api-Version": "2022-11-28",
              },
          )

          try:
              with urllib.request.urlopen(req, timeout=20) as resp:
                  payload = json.loads(resp.read().decode("utf-8"))
          except Exception as exc:
              print(f"Unable to query prior artifacts ({exc}); skipping checkpoint restore.")
              raise SystemExit(0)

          artifacts = payload.get("artifacts", []) if isinstance(payload, dict) else []
          def _artifact_is_candidate(item: object) -> bool:
              if not isinstance(item, dict):
                  return False
              if item.get("expired", True) or not item.get("archive_download_url"):
                  return False
              workflow_run = item.get("workflow_run")
              if not isinstance(workflow_run, dict):
                  return False
              if current_run_id and str(workflow_run.get("id", "")) == current_run_id:
                  return False
              if ref_name and str(workflow_run.get("head_branch", "")) != ref_name:
                  return False
              if str(workflow_run.get("event", "")) != "push":
                  return False
              return True

          artifact = next((item for item in artifacts if _artifact_is_candidate(item)), None)
          if artifact is None:
              print(
                  "No reusable same-branch dataflow-report artifact found; continuing without checkpoint."
              )
              raise SystemExit(0)

          download_url = str(artifact["archive_download_url"])
          req_zip = urllib.request.Request(
              download_url,
              headers={
                  "Accept": "application/vnd.github+json",
                  "Authorization": f"Bearer {token}",
                  "X-GitHub-Api-Version": "2022-11-28",
              },
          )

          try:
              with urllib.request.urlopen(req_zip, timeout=60) as resp:
                  archive_bytes = resp.read()
          except Exception as exc:
              print(f"Unable to download prior artifact ({exc}); skipping checkpoint restore.")
              raise SystemExit(0)

          wanted = {
              "dataflow_resume_checkpoint_ci.json",
          }
          target_chunks = "dataflow_resume_checkpoint_ci.json.chunks/"
          out_dir = Path("artifacts/audit_reports")
          out_dir.mkdir(parents=True, exist_ok=True)
          restored = 0
          with zipfile.ZipFile(io.BytesIO(archive_bytes)) as zf:
              for name in zf.namelist():
                  base = name.split("/", 1)[-1]
                  if base in wanted or base.startswith(target_chunks):
                      if name.endswith("/"):
                          continue
                      dest = out_dir / base
                      dest.parent.mkdir(parents=True, exist_ok=True)
                      dest.write_bytes(zf.read(name))
                      restored += 1

          if restored:
              print(f"Restored {restored} checkpoint artifact file(s) from prior run.")
          else:
              print("Prior artifact did not include resume checkpoint files; continuing without restore.")
          PYTHON
      - name: Dataflow audit staged retries (A->B->C)
        id: dataflow_stage
        env:
          GABION_DIRECT_RUN: "1"
        run: |
          .venv/bin/python scripts/run_dataflow_stage.py --max-attempts 3
      - name: Finalize dataflow audit outcome
        if: always()
        env:
          TERMINAL_EXIT: ${{ steps.dataflow_stage.outputs.exit_code }}
          TERMINAL_STATE: ${{ steps.dataflow_stage.outputs.analysis_state }}
          TERMINAL_STAGE: ${{ steps.dataflow_stage.outputs.terminal_stage }}
          TERMINAL_STATUS: ${{ steps.dataflow_stage.outputs.terminal_status }}
          ATTEMPTS_RUN: ${{ steps.dataflow_stage.outputs.attempts_run }}
        run: |
          terminal_stage="${TERMINAL_STAGE:-none}"
          terminal_exit="${TERMINAL_EXIT:-}"
          terminal_state="${TERMINAL_STATE:-none}"
          terminal_status="${TERMINAL_STATUS:-unknown}"
          attempts_run="${ATTEMPTS_RUN:-0}"
          if [ -z "$terminal_exit" ]; then
            echo "No dataflow audit stage produced an exit code."
            exit 1
          fi
          if [ "$terminal_status" = "unknown" ]; then
            if [ "$terminal_exit" = "0" ]; then
              terminal_status="success"
            elif [ "$terminal_state" = "timed_out_progress_resume" ]; then
              terminal_status="timeout_resume"
            else
              terminal_status="hard_failure"
            fi
          fi
          echo "terminal_stage=$terminal_stage attempts=$attempts_run exit_code=$terminal_exit analysis_state=$terminal_state status=$terminal_status"
          if [ "$terminal_status" = "success" ]; then
            exit 0
          fi
          if [ -f artifacts/audit_reports/dataflow_report.md ]; then
            echo "===== dataflow report ====="
            cat artifacts/audit_reports/dataflow_report.md
          fi
          if [ -f artifacts/audit_reports/timeout_progress.md ]; then
            echo "===== timeout progress ====="
            cat artifacts/audit_reports/timeout_progress.md
          fi
          if [ "$terminal_status" = "timeout_resume" ]; then
            echo "Dataflow audit exhausted timeout budget after staged retries."
            exit 1
          fi
          echo "Dataflow audit failed for a non-timeout reason."
          exit 1
      - name: Deadline profile summary
        if: always()
        run: |
          if [ -f artifacts/out/deadline_profile.json ]; then
            .venv/bin/python scripts/deadline_profile_ci_summary.py \
              --allow-missing-local \
              --step-summary "$GITHUB_STEP_SUMMARY"
          else
            echo "Skipping deadline profile summary (missing artifacts/out/deadline_profile.json)." \
              | tee -a "$GITHUB_STEP_SUMMARY"
          fi
      - name: Upload dataflow report
        if: always()
        uses: actions/upload-artifact@ea165f8d65b6e75b540449e92b4886f43607fa02
        with:
          name: dataflow-report
          path: |
            artifacts/audit_reports/dataflow_report.md
            artifacts/audit_reports/dataflow_report_stage_*.md
            artifacts/audit_reports/timeout_progress.md
            artifacts/audit_reports/timeout_progress.json
            artifacts/audit_reports/timeout_progress_stage_*.md
            artifacts/audit_reports/timeout_progress_stage_*.json
            artifacts/audit_reports/dataflow_resume_checkpoint_ci.json
            artifacts/audit_reports/dataflow_resume_checkpoint_ci.json.chunks/**
            artifacts/out/deadline_profile.json
            artifacts/out/deadline_profile.md
            artifacts/out/deadline_profile_stage_*.json
            artifacts/out/deadline_profile_stage_*.md
            artifacts/out/deadline_profile_ci_summary.json
            artifacts/out/deadline_profile_ci_summary.md

  checks:
    if: github.event_name != 'workflow_dispatch' || (github.ref == 'refs/heads/stage' && github.actor == github.repository_owner)
    runs-on: ubuntu-latest
    env:
      MISE_TRUSTED_CONFIG_PATHS: ${{ github.workspace }}
    steps:
      # Allow-listed actions: actions/checkout, jdx/mise-action
      - name: Checkout
        uses: actions/checkout@de0fac2e4500dabe0009e67214ff5f5447ce83dd
        with:
          fetch-depth: 0
      - name: Install mise
        uses: jdx/mise-action@6d1e696aa24c1aa1bcc1adea0212707c71ab78a8
      - name: Install toolchain
        run: mise install
      - name: Create venv
        run: |
          mise exec -- python -m venv .venv
          echo "VIRTUAL_ENV=$PWD/.venv" >> $GITHUB_ENV
          echo "$PWD/.venv/bin" >> $GITHUB_PATH
      - name: Install dependencies (locked)
        run: |
          .venv/bin/python -m pip install --upgrade pip uv
          .venv/bin/uv pip sync requirements.lock
          .venv/bin/uv pip install -e .
      - name: Policy check (workflows)
        run: .venv/bin/python scripts/policy_check.py --workflows
      - name: Policy check (posture)
        if: github.event_name == 'push'
        env:
          POLICY_GITHUB_TOKEN: ${{ secrets.POLICY_GITHUB_TOKEN }}
        run: |
          if [ -z "${POLICY_GITHUB_TOKEN:-}" ]; then
            echo "POLICY_GITHUB_TOKEN not set; skipping posture check."
            exit 0
          fi
          .venv/bin/python scripts/policy_check.py --posture
      - name: Docflow audit
        run: .venv/bin/python -m gabion docflow-audit --root . --fail-on-violations --sppf-gh-ref-mode required
      - name: SPPF status drift audit
        run: .venv/bin/python scripts/sppf_status_audit.py --root .
      - name: SPPF issue lifecycle validation (non-mutating)
        if: github.event_name == 'push'
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          BEFORE_SHA: ${{ github.event.before }}
          AFTER_SHA: ${{ github.sha }}
        run: |
          if [ -z "${GH_TOKEN:-}" ]; then
            echo "GH_TOKEN is unavailable; skipping SPPF issue lifecycle validation."
            exit 0
          fi
          if [ -z "${BEFORE_SHA:-}" ] || [ "$BEFORE_SHA" = "0000000000000000000000000000000000000000" ]; then
            rev_range="HEAD~20..HEAD"
          else
            if ! git cat-file -e "${AFTER_SHA}^{commit}" 2>/dev/null || ! git cat-file -e "${BEFORE_SHA}^{commit}" 2>/dev/null; then
              git fetch --no-tags origin "$BEFORE_SHA" "$AFTER_SHA" || true
            fi

            if git cat-file -e "${AFTER_SHA}^{commit}" 2>/dev/null && git cat-file -e "${BEFORE_SHA}^{commit}" 2>/dev/null; then
              rev_range="$BEFORE_SHA..$AFTER_SHA"
            else
              echo "Push SHAs unavailable locally; falling back to safe local range."
              rev_range="HEAD~20..HEAD"
            fi
          fi
          .venv/bin/python scripts/sppf_sync.py \
            --validate \
            --only-when-relevant \
            --range "$rev_range" \
            --require-state open \
            --require-label done-on-stage \
            --require-label status/pending-release
      - name: Test evidence index
        env:
          GABION_LSP_TIMEOUT_TICKS: "300000"
          GABION_LSP_TIMEOUT_TICK_NS: "1000000"
        run: |
          .venv/bin/python scripts/extract_test_evidence.py --root . --tests tests --out out/test_evidence.json
          git diff --exit-code out/test_evidence.json
      - name: Tests
        run: |
          mkdir -p artifacts/test_runs
          .venv/bin/python -m pytest \
            --cov=src/gabion \
            --cov-report=term-missing \
            --cov-report=xml:artifacts/test_runs/coverage.xml \
            --cov-report=html:artifacts/test_runs/htmlcov \
            --cov-fail-under=100 \
            --junitxml artifacts/test_runs/junit.xml \
            --log-file artifacts/test_runs/pytest.log \
            --log-file-level=INFO
      - name: Delta state emit
        run: .venv/bin/python scripts/delta_state_emit.py
      - name: Delta triplets (default-on burn-down gates)
        run: .venv/bin/python scripts/delta_triplets.py
      - name: Upload test artifacts
        if: always()
        uses: actions/upload-artifact@ea165f8d65b6e75b540449e92b4886f43607fa02
        with:
          name: test-runs
          path: artifacts/test_runs
