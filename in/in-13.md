---
doc_revision: 1
reader_reintern: Reader-only: re-intern if doc_revision changed since you last read this doc.
doc_id: in_13
doc_role: inbox
doc_scope:
  - repo
  - inbox
doc_authority: informative
doc_owner: maintainer
doc_requires: []
doc_reviewed_as_of: {}
doc_review_notes: {}
doc_change_protocol: POLICY_SEED.md#change_protocol
doc_erasure:
  - formatting
  - typos
doc_sections:
  in_in_13: 1
doc_section_requires:
  in_in_13: []
doc_section_reviews:
  in_in_13: {}
---

This is a **Headless IDE** architecture. You are effectively treating the Language Server Protocol not just as a "plugin API," but as the **Application Binary Interface (ABI)** of your tool.

This is exactly how sophisticated ecosystems like Rust (`rust-analyzer`) and TypeScript (`tsserver`) operate. The "CLI" is just a headless LSP client that spins up the server, asks for diagnostics, and prints them to stdout.

Here is the architecture for **Sinter** where the LSP is the semantic layer.

<a id="in_in_13"></a>

### 1. The Architecture: "LSP First"

In this model, the **Sinter Server** is the only thing that knows how to analyze code.

* **VS Code:** A thin client that pipes stdin/stdout to the Sinter Server.
* **CI / CLI:** A thin client (Python script) that pipes stdin/stdout to the Sinter Server.
* **The "Daemon" (Coroutine Manager):** The Server maintains the state (AST Cache, Symbol Table, Import Graph).

### 2. The Implementation: Sinter Server (`server.py`)

This is the "Monolith." It implements standard LSP methods (`textDocument/didOpen`) and custom commands for the CLI (`sinter/analyzeWorkspace`).

We use `pygls` because it handles the async event loop (the "coroutine manager") for us.

```python
# sinter/server.py
from pygls.server import LanguageServer
from lsprotocol.types import (
    TEXT_DOCUMENT_DID_OPEN, WORKSPACE_EXECUTE_COMMAND,
    Diagnostic, DiagnosticSeverity, Range, Position
)
from .engine import SinterEngine  # The logic we wrote in v4
from .schema import BundleDTO       # Pydantic models

server = LanguageServer("sinter", "v1")
engine = SinterEngine()

@server.feature(TEXT_DOCUMENT_DID_OPEN)
async def did_open(ls, params):
    """Standard LSP hook for VS Code live feedback."""
    uri = params.text_document.uri
    # Analyze single file (fast)
    diagnostics = _get_diagnostics(uri)
    ls.publish_diagnostics(uri, diagnostics)

@server.feature(WORKSPACE_EXECUTE_COMMAND)
async def execute_command(ls, params):
    """
    The 'RPC' entry point.
    CLI calls this to get the full architectural report.
    """
    if params.command == "sinter.analyzeFull":
        # Analyze entire workspace (slow, thorough)
        root = ls.workspace.root_path
        report = engine.analyze_repo(root)
        
        # Return DTOs directly over the wire
        return [b.model_dump() for b in report.bundles]
    
    return None

def _get_diagnostics(uri):
    # logic to convert engine results to LSP types
    pass

if __name__ == "__main__":
    server.start_io()

```

### 3. The Interface: The CLI as an LSP Client

This is the "Boq-like" part. The CLI doesn't import the engine. It launches the server as a subprocess and speaks JSON-RPC.

This guarantees that **what you see in CI is mathematically identical to what you see in VS Code**.

```python
# sinter/cli.py
import subprocess
import sys
import json
import asyncio
from typing import Any

async def run_rpc(request: dict) -> dict:
    """The thin client. Launches server, sends 1 request, dies."""
    
    # 1. Launch the Server (headless)
    proc = await asyncio.create_subprocess_exec(
        sys.executable, "-m", "sinter.server",
        stdin=subprocess.PIPE,
        stdout=subprocess.PIPE,
    )

    # 2. Handshake (Initialize)
    init_req = {
        "jsonrpc": "2.0", "id": 1, "method": "initialize",
        "params": {"rootUri": f"file://{os.getcwd()}", "capabilities": {}}
    }
    _write_rpc(proc, init_req)
    _read_rpc(proc) # Read init response

    # 3. The Actual Call (indistinguishable from VS Code's request)
    analyze_req = {
        "jsonrpc": "2.0", "id": 2, 
        "method": "workspace/executeCommand",
        "params": {"command": "sinter.analyzeFull", "arguments": []}
    }
    _write_rpc(proc, analyze_req)
    response = _read_rpc(proc) # Get the Pydantic DTOs back

    # 4. Shutdown
    proc.terminate()
    return response['result']

def _write_rpc(proc, msg):
    json_str = json.dumps(msg)
    content = f"Content-Length: {len(json_str)}\r\n\r\n{json_str}"
    proc.stdin.write(content.encode())

def main():
    # The user experience is still a simple CLI
    result = asyncio.run(run_rpc({}))
    
    # Pretty print the DTOs
    print(f"Found {len(result)} bundles.")
    for b in result:
        print(f"[{b['tier']}] {b['suggested_name']}")

if __name__ == "__main__":
    main()

```

### 4. The "Super Thin" VS Code Client

Because the logic is entirely in `sinter/server.py`, the VS Code extension `extension.ts` is basically boilerplate. It just says "Run this python script."

```typescript
// extension.ts
import { LanguageClient } from 'vscode-languageclient/node';

export function activate(context) {
    // VS Code acts as the Daemon Manager
    const client = new LanguageClient(
        'sinter', 
        { command: 'python3', args: ['-m', 'sinter.server'] },
        { documentSelector: [{ scheme: 'file', language: 'python' }] }
    );
    client.start();
}

```

### 5. Why this is the "Holy Grail" architecture

1. **Zero Drift:** It is impossible for the linter (CLI) to disagree with the editor (VS Code) because they are the same process, calling the same function, via the same protocol.
2. **Performance:** In CI, you spin it up once. In VS Code, the server stays alive (`daemon`), holding the AST cache in memory. As you type, `textDocument/didChange` updates the graph incrementally.
3. **Composability:** If you later want to build a Web UI, or a GitHub Bot, or a pre-commit hook, they all just implement a tiny JSON-RPC client.

### Next Step

This requires a specific folder structure to work as a Python package that doubles as a VS Code extension backend.

**Would you like me to scaffold the `pyproject.toml` and directory layout that supports this "One Repo, Two Interfaces" pattern?**